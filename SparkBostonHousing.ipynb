{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkBostonHousing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jianzhiw/SparkML/blob/master/SparkBostonHousing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-CrPkfLNn6D",
        "colab_type": "text"
      },
      "source": [
        "# Google Colab #\n",
        "\n",
        "Google colab is a Google Research Project created to aid in machine learning, deep learning education and research. It's a Jupyter notebook environment that requires no setup to use and runs entirely in the cloud.\n",
        "\n",
        "Requirement:\n",
        "1. Google Account\n",
        "2. Internet connection\n",
        "\n",
        "Benefits:\n",
        "1. Free of charge\n",
        "2. No setup required\n",
        "3. Cloud computing\n",
        "4. Free GPU for faster computing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LubKUSMNPb-I",
        "colab_type": "text"
      },
      "source": [
        "# How to enable GPU #\n",
        "\n",
        "Go to Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU\n",
        "\n",
        "![alt text](https://i.imgur.com/hQi43WH.png)\n",
        "\n",
        "# To keep a copy of this Jupyter notebook #\n",
        "\n",
        "Go to File -> Save a copy in Drive\n",
        "\n",
        "You can now access the notebook in your My Drive -> Colab Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS2Z-dHxQIhX",
        "colab_type": "text"
      },
      "source": [
        "# Let's Get Started with PySpark #\n",
        "\n",
        "Apache Spark was build to analyze Big Data with faster speed. One of the important features that Apache Spark offers is the ability to run the computations in memory. It is also considered to be more efficient than MapReduce for the complex application running on Disk.\n",
        "\n",
        "<br>\n",
        "\n",
        "PySpark is the collaboration of Apache Spark and Python. Apache Spark is an open-source cluster-computing framework, built around speed, ease of use, and streaming analytics whereas Python is a general-purpose, high-level programming language. PySpark is a great language for performing exploratory data analysis at scale, building machine learning pipelines, and creating ETLs for a data platform. If youâ€™re already familiar with Python and libraries such as Pandas, then PySpark is a great language to learn in order to create more scalable analyses and pipelines. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSKgMkddRFkw",
        "colab_type": "text"
      },
      "source": [
        "# The Dataset #\n",
        "\n",
        "In this tutorial we are going to look at Boston Housing dataset. This data was originally a part of UCI Machine Learning Repository and has been removed now. We can also access this data from the scikit-learn library. There are 506 samples and 13 feature variables in this dataset. The objective is to predict the value of prices of the house using the given features. You can click on [this](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html?source=post_page---------------------------) to know more about the dataset. \n",
        "\n",
        "<br><br>\n",
        "\n",
        "Before dive deep into the code, you will need to download this [dataset](https://drive.google.com/file/d/1JdhWZPPClfJwHpOJPDgue_9VDkSrOIA2/view). \n",
        "\n",
        "<br><br>\n",
        "\n",
        "To do so:\n",
        "1. Click on the arrow button on the left\n",
        "2. Select Upload and insert the file\n",
        "3. You are good to go\n",
        "\n",
        "![alt text](https://i.imgur.com/ku133ZN.png)\n",
        "\n",
        "Alternatively, you can use terminal command below to download the file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUQFzwtHUnaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove the hashtag in order to run it\n",
        "# !wget -O BostonHousing.csv https://drive.google.com/uc?id=1JdhWZPPClfJwHpOJPDgue_9VDkSrOIA2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px4Qh3EQXMbs",
        "colab_type": "text"
      },
      "source": [
        "# Running PySpark in Colab # \n",
        "\n",
        "To run spark in Colab, first we need to install all the dependencies in Colab environment such as Apache Spark 2.4.3 with hadoop 2.7, Java 8 and Findspark in order to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MErzN6ouLwLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.3-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii2XQDtyXbPs",
        "colab_type": "text"
      },
      "source": [
        "Now that you installed Spark and Java in Colab, it is time to set the environment path which enables you to run Pyspark in your Colab environment. Set the location of Java and Spark by running the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNMzNhlfNK00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.3-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFrcPuTMXffc",
        "colab_type": "text"
      },
      "source": [
        "Run a local spark session to test your installation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhNdnvTXNVTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dw-uoDyXhW9",
        "colab_type": "text"
      },
      "source": [
        "Your Colab is now ready to run Pyspark. Let's build a simple Linear Regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0mDf8tsXoJ4",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression Model #\n",
        "Linear Regression model is one the oldest and widely used machine learning approach which assumes a relationship between dependent and independent variables. For example, a modeler might want to predict the forecast of the rain based on the humidity ratio. Linear Regression consists of the best fitting line through the scattered points on the graph and the best fitting line is known as the regression line.\n",
        "\n",
        "<br>\n",
        "\n",
        "The goal of this exercise to predict the housing prices by the given features. Let's predict the prices of the Boston Housing dataset by considering MEDV as the output variable and all the other variables as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InEFvVJZX-um",
        "colab_type": "text"
      },
      "source": [
        "For our linear regression model we need to import two modules from Pyspark i.e. Vector Assembler and Linear Regression. Vector Assembler is a transformer that assembles all the features into one vector from multiple columns that contain type double. We could have used StringIndexer if any of our columns contains string values to convert it into numeric values. Luckily, the BostonHousing dataset only contains double values, so we don't need to worry about StringIndexer for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wla5W7VJNZPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "dataset = spark.read.csv('BostonHousing.csv',inferSchema=True, header =True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVfpzAYhYF4y",
        "colab_type": "text"
      },
      "source": [
        "Notice that we used InferSchema inside read.csv mofule. InferSchema enables us to infer automatically different data types for each column.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let us print look into the dataset to see the data types of each column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzyBEdObPZf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.printSchema()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwxAqb3jYQJv",
        "colab_type": "text"
      },
      "source": [
        "Next step is to convert all the features from different columns into a single column and let's call this new vector column as 'Attributes' in the output column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Iyl4xzJPeU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input all the features in one vector column\n",
        "assembler = VectorAssembler(inputCols=['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax', 'ptratio', 'b', 'lstat'], outputCol = 'Attributes')\n",
        "\n",
        "output = assembler.transform(dataset)\n",
        "\n",
        "#Input vs Output\n",
        "finalized_data = output.select(\"Attributes\",\"medv\")\n",
        "\n",
        "finalized_data.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwR8uHpUYW_H",
        "colab_type": "text"
      },
      "source": [
        "Here, 'Attributes' are in the input features from all the columns and 'medv' is the target column. Next, we should split the training and testing data according to our dataset (0.8 and 0.2 in this case)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrRAve95Pt1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split training and testing data\n",
        "train_data,test_data = finalized_data.randomSplit([0.8,0.2])\n",
        "\n",
        "\n",
        "regressor = LinearRegression(featuresCol = 'Attributes', labelCol = 'medv')\n",
        "\n",
        "#Learn to fit the model from training set\n",
        "regressor = regressor.fit(train_data)\n",
        "\n",
        "#To predict the prices on testing set\n",
        "pred = regressor.evaluate(test_data)\n",
        "\n",
        "#Predict the model\n",
        "pred.predictions.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OBdgR9Yc9b",
        "colab_type": "text"
      },
      "source": [
        "We can also print the coefficient and intercept of the regression model by using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqK3gl41Pwhv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#coefficient of the regression model\n",
        "coeff = regressor.coefficients\n",
        "\n",
        "#X and Y intercept\n",
        "intr = regressor.intercept\n",
        "\n",
        "print (\"The coefficient of the model is : %a\" %coeff)\n",
        "print (\"The Intercept of the model is : %f\" %intr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBqiCXD9YhNc",
        "colab_type": "text"
      },
      "source": [
        "# Basic Statistical Analysis #\n",
        "Once we are done with the basic linear regression operation, we can go a bit further and analyze our model statistically by importing RegressionEvaluator module from Pyspark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzDwIpLXP0Sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "eval = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "\n",
        "# Root Mean Square Error\n",
        "rmse = eval.evaluate(pred.predictions)\n",
        "print(\"RMSE: %.3f\" % rmse)\n",
        "\n",
        "# Mean Square Error\n",
        "mse = eval.evaluate(pred.predictions, {eval.metricName: \"mse\"})\n",
        "print(\"MSE: %.3f\" % mse)\n",
        "\n",
        "# Mean Absolute Error\n",
        "mae = eval.evaluate(pred.predictions, {eval.metricName: \"mae\"})\n",
        "print(\"MAE: %.3f\" % mae)\n",
        "\n",
        "# r2 - coefficient of determination\n",
        "r2 = eval.evaluate(pred.predictions, {eval.metricName: \"r2\"})\n",
        "print(\"r2: %.3f\" %r2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52bIbxN9YmGw",
        "colab_type": "text"
      },
      "source": [
        "# Source #\n",
        "\n",
        "[PySpark in Colab](https://github.com/asifahmed90/pyspark-ML-in-Colab)\n",
        "\n",
        "[Boston Housing Dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html?source=post_page---------------------------)"
      ]
    }
  ]
}